{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## How to create a Deep Ensemble for MNIST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import uncertainty_wizard as uwiz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Step 1: Downloading Preprocessing the data**\n",
    "\n",
    "This is the same that we would do on any regular keras mnist classifier,\n",
    "except that we do not have to one-hot encode the test labels, as uncertainty wizards quantifiers\n",
    "will determine the winning class (one not its one-hot encoding) for us"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lets cache the train data on the file system,\n",
    "# and at the same time also prepare the test data for later\n",
    "_,(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_test = (x_test.astype('float32') / 255).reshape(x_test.shape[0], 28, 28, 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Step 2: Creating a Stochastic Uncertainty-Wizard Model using the Sequential API**\n",
    "\n",
    "Note that only the first line is different from doing the same in plain keras, the rest is equivalent.\n",
    "We must however ensure to use at least one randomized layer (e.g. tf.keras.layers.Dropout)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's create an uncertainty wizard ensemble!\n",
    "\n",
    "#   First, we define the model setup and training process\n",
    "#   This is done using traditional tensorflow code\n",
    "\n",
    "def model_creation_and_training(model_id: int):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "    # Note that we set the number of epochs to just 1, to be able to run this notebook quickly\n",
    "    # Set the number of epochs higher if you want to optimally train the network\n",
    "    (x_train, y_train), (_,_) = tf.keras.datasets.mnist.load_data()\n",
    "    x_train = (x_train.astype('float32') / 255).reshape(x_train.shape[0], 28, 28, 1)\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "    fit_history = model.fit(x_train, y_train, validation_split=0.1, batch_size=32, epochs=5,\n",
    "                      verbose=1, callbacks=[tf.keras.callbacks.EarlyStopping(patience=2)])\n",
    "    return model, fit_history.history\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Now we can define and train our ensemble model.\n",
    "# Here we use two models as its faster to train in this example. In practice, you may want to use more.\n",
    "ensemble = uwiz.models.LazyEnsemble(num_models=2,\n",
    "                                    model_save_path=\"/tmp/ensemble\",\n",
    "                                    # Colab infrastructure is relatively weak.\n",
    "                                    # Thus, lets disable multiprocessing and train on the main process.\n",
    "                                    # Any argument >= 1 would result in (typically more efficient) multiprocessing\n",
    "                                    # on a more powerful machine\n",
    "                                    default_num_processes=0)\n",
    "\n",
    "ensemble.create(create_function=model_creation_and_training)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Step 3: Make predictions and get the uncertainties and confidences**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get two one-dimensional np arrays: One containing the predictions and one containing the confidences\n",
    "predictions, confidences = ensemble.predict_quantified(x_test,\n",
    "                                                       quantifier='mean_softmax')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}