{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## How to create a Deep Ensemble for MNIST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "  import uncertainty_wizard as uwiz\n",
    "except ModuleNotFoundError as e:\n",
    "  # Uncertainty wizard was not installed. Install it now (we're probably on colab)\n",
    "  !pip install uncertainty_wizard\n",
    "  import uncertainty_wizard as uwiz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Step 1: Downloading Preprocessing the data**\n",
    "\n",
    "This is the same that we would do on any regular keras mnist classifier,\n",
    "except that we do not have to one-hot encode the test labels, as uncertainty wizards quantifiers\n",
    "will determine the winning class for us"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lets cache the train data on the file system,\n",
    "# and at the same time also prepare the test data for later\n",
    "_,(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_test = (x_test.astype('float32') / 255).reshape(x_test.shape[0], 28, 28, 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Step 2: Define the model creation & training process in a picklable function**\n",
    "\n",
    "Just create a function at the root of your file, using plain tensorflow code. \n",
    "The function should return a newly created model and a second return value (typically the training history).\n",
    "\n",
    "This function will be called repetitively to create the atomic models in the ensemble.\n",
    "The optional return values will be collected and returned after the creation of the ensemble."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def model_creation_and_training(model_id: int):\n",
    "    import tensorflow as tf\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "    (x_train, y_train), (_,_) = tf.keras.datasets.mnist.load_data()\n",
    "    x_train = (x_train.astype('float32') / 255).reshape(x_train.shape[0], 28, 28, 1)\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "    # Note that we set the number of epochs to just 1, to be able to run this notebook quickly\n",
    "    # Set the number of epochs higher if you want to optimally train the network\n",
    "    fit_history = model.fit(x_train, y_train, validation_split=0.1, batch_size=32, epochs=1,\n",
    "                      verbose=1, callbacks=[tf.keras.callbacks.EarlyStopping(patience=2)])\n",
    "    return model, fit_history.history\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Step 3: Create Ensemble**\n",
    "\n",
    "Let's create a Lazy Ensemble instance, i.e., a definition of how many atomic models should be included in our ensemble,\n",
    "where they should be persisted, ... Note that this first call does not create or train any models and is thus super fast.\n",
    "\n",
    "After this definition, we can create the atomic models in the lazy ensemble using the function defined above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ensemble = uwiz.models.LazyEnsemble(num_models=2, # For the sake of this example. Use more in practice!\n",
    "                                    model_save_path=\"/tmp/ensemble\",\n",
    "                                    # Colab infrastructure is relatively weak.\n",
    "                                    # Thus, lets disable multiprocessing and train on the main process.\n",
    "                                    # Any argument >= 1 would result in (typically more efficient) multiprocessing\n",
    "                                    # on a more powerful machine\n",
    "                                    default_num_processes=0)\n",
    "# Creates, trains and persists atomic models using our function defined above\n",
    "training_histories = ensemble.create(create_function=model_creation_and_training)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Step 4: Make predictions and get the uncertainties and confidences**\n",
    "\n",
    "If your test data is a numpy array, its as easy as shown in the code below.\n",
    "\n",
    "For customized prediction procedures, \n",
    "or a non-numpy test set, check out the documentation for ensemble.quantify_predictions where you\n",
    "can hook up an arbitrary prediction function - similar to the training function defined and used in step 2 and 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get two one-dimensional np arrays: One containing the predictions and one containing the confidences\n",
    "predictions, confidences = ensemble.predict_quantified(x_test,\n",
    "                                                       quantifier='mean_softmax')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}